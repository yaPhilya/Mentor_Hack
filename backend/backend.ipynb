{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import functools # for partial parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim # td-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn # sentence similarity with wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Пример сравнения похожести документов (Тупой, через TD-IDF статистики)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = [\"I'm taking the show on the road.\",\n",
    "                 \"My socks are a force multiplier.\",\n",
    "             \"I am the barber who cuts everyone's hair who doesn't cut their own.\",\n",
    "             \"Legend has it that the mind is a mad monkey.\",\n",
    "            \"I make my own fun.\"]\n",
    "print(\"Number of documents:\",len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in raw_documents]\n",
    "print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary[5])\n",
    "print(dictionary.token2id['road'])\n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "for i in range(len(dictionary)):\n",
    "    print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a corpus. A corpus is a list of bags of words. A bag-of-words representation for a document just lists the number of times each word occurs in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое TF-IDF\n",
    "\n",
    "http://nlpx.net/archives/57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "s = 0\n",
    "for i in corpus:\n",
    "    s += len(i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a similarity measure object in tf-idf space.\n",
    "tf-idf stands for term frequency-inverse document frequency. Term frequency is how often the word shows up in the document and inverse document fequency scales the value by how rare the word is in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('./workdir/',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a query document and convert it to tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc = [w.lower() for w in word_tokenize(\"Socks are a force for good.\")]\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show an array of document similarities to query. We see that the second document is the most similar with the overlapping of socks and force."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Схожесть предложений с использованием Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check out how frequent a lemma is by doing:\n",
    "cat = wn.synsets('cat', 'n')[0]     # Get the most common synset\n",
    "print (cat.lemmas()[0].count()  )     # Get the first lemma => 18\n",
    " \n",
    "dog = wn.synsets('dog', 'n')[0]           # Get the most common synset\n",
    "feline = wn.synsets('feline', 'n')[0]     # Get the most common synset\n",
    "mammal = wn.synsets('mammal', 'n')[0]     # Get the most common synset\n",
    " \n",
    "# You can read more about the different types of wordnet similarity measures here: http://www.nltk.org/howto/wordnet.html\n",
    "for synset in [dog, feline, mammal]:\n",
    "    print (\"Similarity(%s, %s) = %s\" % (cat, synset, cat.wup_similarity(synset)))\n",
    " \n",
    "# Similarity(Synset('cat.n.01'), Synset('dog.n.01')) = 0.2\n",
    "# Similarity(Synset('cat.n.01'), Synset('feline.n.01')) = 0.5\n",
    "# Similarity(Synset('cat.n.01'), Synset('mammal.n.01')) = 0.2\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nlpforhackers.io/wordnet-sentence-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(sentence1)#pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(sentence2)#pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        curr_arr = [x for x in [synset.path_similarity(ss) for ss in synsets2] if x is not None]\n",
    "        curr_arr.append(0)\n",
    "#         print(curr_arr)\n",
    "        \n",
    "        best_score = max(curr_arr)\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    "\n",
    "    score /= count\n",
    "    return score\n",
    " \n",
    "sentences = [\n",
    "    \"Programming technologies\",\n",
    "    \"Some gorgeous creatures are felines.\",\n",
    "    \"Dolphins are swimming mammals.\",\n",
    "    \"C++ Is beautiful shit.\",\n",
    "]\n",
    " \n",
    "focus_sentence = \"C++ are beautiful programming language\"\n",
    " \n",
    "for sentence in sentences:\n",
    "    print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence)))\n",
    "    print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (sentence, focus_sentence, sentence_similarity(sentence, focus_sentence)))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Технические теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "technical_tags = pd.read_csv('../DB/most_popular_stack_overflow.csv')\n",
    "technical_tags = set(technical_tags['TagName'])\n",
    "technical_tags = {x for x in technical_tags if x==x}\n",
    "\n",
    "technical_tags_dict = {}\n",
    "technical_tags_ind_dict = {}\n",
    "\n",
    "index = 0\n",
    "for x in technical_tags:\n",
    "    technical_tags_dict[x] = index\n",
    "    technical_tags_ind_dict[index] = x\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Синонимичный ряд замен для тегов технологий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "technical_tags_syn = pd.read_csv('../DB/synonyms_for_stackoverflow.csv')\n",
    "technical_tags_syn.head()\n",
    "\n",
    "technical_tags_syn_dict = dict(zip(technical_tags_syn.SOurceTagName, technical_tags_syn.TargetTagName))\n",
    "# technical_tags_syn.set_index('SOurceTagName')['TargetTagName'].to_dict('index')\n",
    "technical_tags_syn_dict\n",
    "\n",
    "def technical_tags_syn_try_replace(x):\n",
    "    if x in technical_tags_syn_dict.keys():\n",
    "        return technical_tags_syn_dict[x]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подобие семантической сети тегов технологий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_stackoverflow = pd.read_csv('../DB/Small_stackoverflow.csv')\n",
    "small_stackoverflow = small_stackoverflow['Tags']\n",
    "small_stackoverflow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_technologies = pd.DataFrame(index=technical_tags,columns=technical_tags)\n",
    "semantic_technologies = semantic_technologies.fillna(0)\n",
    "semantic_technologies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for line in small_stackoverflow:\n",
    "    currlist = line[1:-1].split('><')\n",
    "    for x in currlist:\n",
    "        for y in currlist:\n",
    "            if ((x in technical_tags) and (y in technical_tags)):\n",
    "                semantic_technologies.set_value(x, y, semantic_technologies.at[x, y] + 1)\n",
    "                \n",
    "semantic_tech_axis_sum0 = semantic_technologies.sum(axis=0)\n",
    "semantic_tech_axis_sum1 = semantic_technologies.sum(axis=1)\n",
    "# semantic_technologies.tail()\n",
    "semantic_tech_axis_sum1.head()\n",
    "\n",
    "\n",
    "for index, row in semantic_technologies.iterrows():\n",
    "#     print(type(index))\n",
    "    semantic_technologies.loc[index] = semantic_technologies.loc[index] /\\\n",
    "    (np.minimum(semantic_tech_axis_sum1[index], semantic_tech_axis_sum0))\n",
    "    semantic_technologies.loc[index, index] = -1.0\n",
    "semantic_technologies.fillna(0,inplace=True)\n",
    "\n",
    "for x in technical_tags:\n",
    "    print('For', x, 'popular tag is', np.argmax(semantic_technologies[x]))\n",
    "# np.argmax(semantic_technologies['html'])\n",
    "# semantic_technologies.loc['html', 'positioning']\n",
    "\n",
    "# print(sorted(list(semantic_technologies['html'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кастомный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_small_set = pd.read_csv('small_custom_dataset.csv', sep=';')\n",
    "custom_small_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общая структура происходящего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = custom_small_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geo_distance_metrics(geo1, geo2):\n",
    "       return abs(geo1 - geo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geo_close_enough(geo1, geo2):\n",
    "    return geo_distance_metrics(geo1, geo2) < 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def closest_people(index):\n",
    "    person = data.loc[index]\n",
    "    \n",
    "    print(\"Out req person:\", person)\n",
    "    \n",
    "    stud_lambd1 = lambda x: \\\n",
    "(geo_close_enough(person.geo, x.geo) or (person.ementoring >= 1 and x.ementoring >= 1)) and \\\n",
    "x.user_role == 'mentor'\n",
    "\n",
    "    ment_lambd1 = lambda x: \\\n",
    "(geo_close_enough(person.geo, x.geo) or (person.ementoring >= 1 and x.ementoring >= 1)) and \\\n",
    "x.user_role == 'student'\n",
    "    \n",
    "    if person.user_role == 'student':\n",
    "        closest_people = data[data.apply(stud_lambd1, axis=1)]\n",
    "        return closest_people\n",
    "    elif person.user_role == 'mentor':\n",
    "        closest_people = data[data.apply(ment_lambd1, axis=1)]\n",
    "        return closest_people\n",
    "        \n",
    "#     return data.loc[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Жесткое косинусное расстояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    print('vec', vec1, vec2)\n",
    "    \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "def list_to_vector(text):\n",
    "    return Counter(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мягкое косинусное расстояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tech_tag_sim(w1, w2):\n",
    "    if (w1 == w2):\n",
    "        return 1.0\n",
    "    elif (w1 in technical_tags and w2 in technical_tags):\n",
    "        return semantic_technologies.loc[w1, w2]\n",
    "    else:\n",
    "        return 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine_soft(vec1, vec2):\n",
    "    print('vec', vec1, vec2)\n",
    "    \n",
    "    S1 = 0\n",
    "    S2 = 0\n",
    "    S3 = 0\n",
    "    for x in vec1:\n",
    "        for y in vec2:\n",
    "            S1 += tech_tag_sim(x, y)\n",
    "    for x in vec1:\n",
    "        for y in vec1:\n",
    "            S2 += tech_tag_sim(x, y)\n",
    "    for x in vec2:\n",
    "        for y in vec2:\n",
    "            S3 += tech_tag_sim(x, y)\n",
    "    \n",
    "    S2 = S2**0.5\n",
    "    S3 = S3**0.5\n",
    "    \n",
    "    if (S2 * S3 > 0):\n",
    "        return S1 / (S2 * S3)\n",
    "    else:\n",
    "        return 0.0\n",
    "#     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    \n",
    "    \n",
    "#     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "#     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "#     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "#     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "#     if not denominator:\n",
    "#         return 0.0\n",
    "#     else:\n",
    "#         return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cosine_soft(['c++'], ['c++'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cosine_soft(['html'], ['css'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cosine_soft(['c++'], ['animals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_person(person, x):\n",
    "    w0 = 0\n",
    "    w1 = 1\n",
    "    xskills = [technical_tags_syn_try_replace(x) for x in x.skills_tags.split(',')] #x.skills_tags.replace(\",\", \" \")\n",
    "    xintro = x.intro_phrase_tags.split(',') # x.intro_phrase_tags.replace(\",\", \" \")#\n",
    "    \n",
    "    personskills = [technical_tags_syn_try_replace(x) for x in person.skills_tags.split(',')] #person.skills_tags.replace(\",\", \" \")\n",
    "    personintro = person.intro_phrase_tags.split(',')#person.intro_phrase_tags.replace(\",\", \" \")#\n",
    "    \n",
    "    x_alltags = xskills + xintro\n",
    "    person_alltags = personskills + personintro\n",
    "    \n",
    "    xtechnical = [x for x in x_alltags if x in technical_tags]\n",
    "    persontechnical = [x for x in person_alltags if x in technical_tags]\n",
    "    xhuman = [x for x in x_alltags if not (x in technical_tags)]\n",
    "    personhuman = [x for x in person_alltags if (not x in technical_tags)]\n",
    "\n",
    "    return w0 * sentence_similarity(xhuman, personhuman) +\\\n",
    "        w1 * get_cosine_soft(list_to_vector(xtechnical), list_to_vector(persontechnical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_req(index):\n",
    "    person = data.loc[index]\n",
    "    \n",
    "    closest1 = closest_people(index)\n",
    "    \n",
    "    sLength = len(closest1['id'])\n",
    "    closest1['tag_score'] = closest1.apply(functools.partial(check_person, person), axis=1)\n",
    "    return closest1.sort_values(by=['tag_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process_req(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cosine(list_to_vector(['c++','java']), list_to_vector(['java','c#']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([(1, 2),(1, 2),(1, 2)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'fieldA' : ['lol', 'kek', 'fuf'], 'fieldB' : ['anus1', 'anus2', 'anus1']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'fieldB' : ['anus1', 'anus2'], 'fieldC' : [100, 200]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on='fieldB', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(index=str, columns={\"fieldB\": \"whore\"})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
